{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8k9rCswcib_2",
        "outputId": "32e7821d-bd8f-4752-bf44-962234bc63f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "nACoHXlqhUx6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import keras_tuner as kt\n",
        "import tensorflow_datasets as tfds\n",
        "import time\n",
        "\n",
        "import sys\n",
        "import os\n",
        "py_file_location = \"/content/drive/My Drive\"\n",
        "sys.path.append(os.path.abspath('/content/drive/MyDrive/NeuralNetwork'))\n",
        "from models_hs import *\n",
        "from trainer import *\n",
        "from preprocess import *\n",
        "from callbacks import *\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QT5EH_FOmWOO"
      },
      "source": [
        "# 0 Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNCVZl86MyYb"
      },
      "source": [
        "Read in data for each dimension and store as `pd.DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "VhvAhRHNl0T7"
      },
      "outputs": [],
      "source": [
        "def get_dfs(dims, num_samples):\n",
        "  dataframes_list = []\n",
        "  for dim in dims:\n",
        "      temp_df = pd.read_csv(f\"/content/drive/MyDrive/data/{num_samples}_basket_data_{dim}.csv\")\n",
        "      temp_df = temp_df.drop(['Unnamed: 0'], axis=1)\n",
        "\n",
        "      # move column with contract prices to the end\n",
        "      cols = list(temp_df.columns.values) \n",
        "      cols.pop(cols.index('contract_price')) \n",
        "      temp_df = temp_df[cols+['contract_price']] \n",
        "\n",
        "      dataframes_list.append(temp_df)\n",
        "  return dataframes_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "ppWrnZqGspYq",
        "outputId": "3f2a22c2-85f0-41d8-907b-8007be3fcb89"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-acdc9da1-e35e-4972-b8fe-b64976415fb0\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>days_to_maturity</th>\n",
              "      <th>strike</th>\n",
              "      <th>volatility</th>\n",
              "      <th>mean_volatility</th>\n",
              "      <th>reversion</th>\n",
              "      <th>vol_of_var</th>\n",
              "      <th>rate</th>\n",
              "      <th>Underlying_0</th>\n",
              "      <th>Rho_0</th>\n",
              "      <th>contract_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>52.0</td>\n",
              "      <td>16.50</td>\n",
              "      <td>0.010564</td>\n",
              "      <td>0.041048</td>\n",
              "      <td>1.775966</td>\n",
              "      <td>0.058849</td>\n",
              "      <td>0.015002</td>\n",
              "      <td>17.332500</td>\n",
              "      <td>-0.333905</td>\n",
              "      <td>10.938264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>21.0</td>\n",
              "      <td>49.50</td>\n",
              "      <td>0.039117</td>\n",
              "      <td>0.049353</td>\n",
              "      <td>1.540456</td>\n",
              "      <td>0.489135</td>\n",
              "      <td>0.001440</td>\n",
              "      <td>9.260000</td>\n",
              "      <td>-0.129997</td>\n",
              "      <td>0.571392</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>31.0</td>\n",
              "      <td>14.00</td>\n",
              "      <td>0.029249</td>\n",
              "      <td>0.006491</td>\n",
              "      <td>0.982883</td>\n",
              "      <td>0.224651</td>\n",
              "      <td>0.017221</td>\n",
              "      <td>16.414301</td>\n",
              "      <td>-0.462886</td>\n",
              "      <td>8.445650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8.70</td>\n",
              "      <td>0.038737</td>\n",
              "      <td>0.016305</td>\n",
              "      <td>2.372708</td>\n",
              "      <td>0.329505</td>\n",
              "      <td>0.006641</td>\n",
              "      <td>15.892000</td>\n",
              "      <td>-0.141496</td>\n",
              "      <td>9.129831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>59.0</td>\n",
              "      <td>16.50</td>\n",
              "      <td>0.043640</td>\n",
              "      <td>0.038880</td>\n",
              "      <td>4.562963</td>\n",
              "      <td>0.706960</td>\n",
              "      <td>0.053901</td>\n",
              "      <td>11.277000</td>\n",
              "      <td>-0.379364</td>\n",
              "      <td>10.687387</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>995</th>\n",
              "      <td>38.0</td>\n",
              "      <td>13.00</td>\n",
              "      <td>0.001502</td>\n",
              "      <td>0.046531</td>\n",
              "      <td>1.663993</td>\n",
              "      <td>0.145838</td>\n",
              "      <td>0.001362</td>\n",
              "      <td>12.819120</td>\n",
              "      <td>-0.307483</td>\n",
              "      <td>6.298129</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>996</th>\n",
              "      <td>85.0</td>\n",
              "      <td>15.00</td>\n",
              "      <td>0.042181</td>\n",
              "      <td>0.044124</td>\n",
              "      <td>0.302489</td>\n",
              "      <td>0.661890</td>\n",
              "      <td>0.018707</td>\n",
              "      <td>18.106250</td>\n",
              "      <td>-0.050286</td>\n",
              "      <td>24.277845</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>997</th>\n",
              "      <td>29.0</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.033398</td>\n",
              "      <td>0.005511</td>\n",
              "      <td>4.443828</td>\n",
              "      <td>0.506015</td>\n",
              "      <td>0.002109</td>\n",
              "      <td>67.483701</td>\n",
              "      <td>-0.693189</td>\n",
              "      <td>65.758164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>998</th>\n",
              "      <td>7.0</td>\n",
              "      <td>9.50</td>\n",
              "      <td>0.004519</td>\n",
              "      <td>0.036562</td>\n",
              "      <td>2.277263</td>\n",
              "      <td>0.288585</td>\n",
              "      <td>0.003034</td>\n",
              "      <td>15.919620</td>\n",
              "      <td>-0.470891</td>\n",
              "      <td>7.345720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>999</th>\n",
              "      <td>28.0</td>\n",
              "      <td>15.55</td>\n",
              "      <td>0.024088</td>\n",
              "      <td>0.020993</td>\n",
              "      <td>2.809961</td>\n",
              "      <td>0.701111</td>\n",
              "      <td>0.002110</td>\n",
              "      <td>16.374000</td>\n",
              "      <td>-0.348144</td>\n",
              "      <td>7.016429</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-acdc9da1-e35e-4972-b8fe-b64976415fb0')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-acdc9da1-e35e-4972-b8fe-b64976415fb0 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-acdc9da1-e35e-4972-b8fe-b64976415fb0');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "     days_to_maturity  strike  volatility  mean_volatility  reversion  \\\n",
              "0                52.0   16.50    0.010564         0.041048   1.775966   \n",
              "1                21.0   49.50    0.039117         0.049353   1.540456   \n",
              "2                31.0   14.00    0.029249         0.006491   0.982883   \n",
              "3                18.0    8.70    0.038737         0.016305   2.372708   \n",
              "4                59.0   16.50    0.043640         0.038880   4.562963   \n",
              "..                ...     ...         ...              ...        ...   \n",
              "995              38.0   13.00    0.001502         0.046531   1.663993   \n",
              "996              85.0   15.00    0.042181         0.044124   0.302489   \n",
              "997              29.0    0.48    0.033398         0.005511   4.443828   \n",
              "998               7.0    9.50    0.004519         0.036562   2.277263   \n",
              "999              28.0   15.55    0.024088         0.020993   2.809961   \n",
              "\n",
              "     vol_of_var      rate  Underlying_0     Rho_0  contract_price  \n",
              "0      0.058849  0.015002     17.332500 -0.333905       10.938264  \n",
              "1      0.489135  0.001440      9.260000 -0.129997        0.571392  \n",
              "2      0.224651  0.017221     16.414301 -0.462886        8.445650  \n",
              "3      0.329505  0.006641     15.892000 -0.141496        9.129831  \n",
              "4      0.706960  0.053901     11.277000 -0.379364       10.687387  \n",
              "..          ...       ...           ...       ...             ...  \n",
              "995    0.145838  0.001362     12.819120 -0.307483        6.298129  \n",
              "996    0.661890  0.018707     18.106250 -0.050286       24.277845  \n",
              "997    0.506015  0.002109     67.483701 -0.693189       65.758164  \n",
              "998    0.288585  0.003034     15.919620 -0.470891        7.345720  \n",
              "999    0.701111  0.002110     16.374000 -0.348144        7.016429  \n",
              "\n",
              "[1000 rows x 10 columns]"
            ]
          },
          "execution_count": 130,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dims = [1, 4, 7, 10, 13, 16]\n",
        "dataframes_list = get_dfs(dims, 1000)\n",
        "dataframes_list[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XzON4-ks14b"
      },
      "source": [
        "Test model on 10000 samples with dim = 1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "EaW4QHOus1TJ",
        "outputId": "f2fa264e-06eb-4934-bf77-dfeb038452a8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-bf9188f3-139d-4e57-9d83-5575bf99ba7d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>days_to_maturity</th>\n",
              "      <th>strike</th>\n",
              "      <th>volatility</th>\n",
              "      <th>mean_volatility</th>\n",
              "      <th>reversion</th>\n",
              "      <th>vol_of_var</th>\n",
              "      <th>rate</th>\n",
              "      <th>Underlying_0</th>\n",
              "      <th>Rho_0</th>\n",
              "      <th>contract_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>24.0</td>\n",
              "      <td>4.250</td>\n",
              "      <td>0.002046</td>\n",
              "      <td>0.050490</td>\n",
              "      <td>0.842923</td>\n",
              "      <td>0.631400</td>\n",
              "      <td>0.022602</td>\n",
              "      <td>58.010400</td>\n",
              "      <td>-0.320407</td>\n",
              "      <td>61.992674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>119.0</td>\n",
              "      <td>5.050</td>\n",
              "      <td>0.050203</td>\n",
              "      <td>0.048482</td>\n",
              "      <td>2.676144</td>\n",
              "      <td>0.702256</td>\n",
              "      <td>0.001616</td>\n",
              "      <td>39.998899</td>\n",
              "      <td>-0.102169</td>\n",
              "      <td>16.377944</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>10.0</td>\n",
              "      <td>15.000</td>\n",
              "      <td>0.012146</td>\n",
              "      <td>0.009763</td>\n",
              "      <td>0.952558</td>\n",
              "      <td>0.693369</td>\n",
              "      <td>0.009901</td>\n",
              "      <td>5.080200</td>\n",
              "      <td>-0.394712</td>\n",
              "      <td>0.026545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>235.0</td>\n",
              "      <td>14.000</td>\n",
              "      <td>0.028765</td>\n",
              "      <td>0.038385</td>\n",
              "      <td>4.946453</td>\n",
              "      <td>0.012569</td>\n",
              "      <td>0.001688</td>\n",
              "      <td>13.283750</td>\n",
              "      <td>-0.471352</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>130.0</td>\n",
              "      <td>16.300</td>\n",
              "      <td>0.010112</td>\n",
              "      <td>0.045151</td>\n",
              "      <td>0.474549</td>\n",
              "      <td>0.218541</td>\n",
              "      <td>0.015604</td>\n",
              "      <td>4.440500</td>\n",
              "      <td>-0.101970</td>\n",
              "      <td>4.166032</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9995</th>\n",
              "      <td>22.0</td>\n",
              "      <td>3.975</td>\n",
              "      <td>0.003293</td>\n",
              "      <td>0.047075</td>\n",
              "      <td>0.377678</td>\n",
              "      <td>0.601305</td>\n",
              "      <td>0.026390</td>\n",
              "      <td>12.496370</td>\n",
              "      <td>-0.421565</td>\n",
              "      <td>11.578677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9996</th>\n",
              "      <td>7.0</td>\n",
              "      <td>14.000</td>\n",
              "      <td>0.022504</td>\n",
              "      <td>0.025393</td>\n",
              "      <td>0.826864</td>\n",
              "      <td>0.429884</td>\n",
              "      <td>0.001874</td>\n",
              "      <td>15.524250</td>\n",
              "      <td>-0.229666</td>\n",
              "      <td>3.513765</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9997</th>\n",
              "      <td>10.0</td>\n",
              "      <td>12.400</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>0.040227</td>\n",
              "      <td>0.190017</td>\n",
              "      <td>0.502809</td>\n",
              "      <td>0.002174</td>\n",
              "      <td>4.612700</td>\n",
              "      <td>-0.243227</td>\n",
              "      <td>0.296504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9998</th>\n",
              "      <td>192.0</td>\n",
              "      <td>4.450</td>\n",
              "      <td>0.044043</td>\n",
              "      <td>0.030118</td>\n",
              "      <td>4.034105</td>\n",
              "      <td>0.391629</td>\n",
              "      <td>0.025649</td>\n",
              "      <td>19.868500</td>\n",
              "      <td>-0.190983</td>\n",
              "      <td>2.275450</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9999</th>\n",
              "      <td>186.0</td>\n",
              "      <td>4.950</td>\n",
              "      <td>0.050072</td>\n",
              "      <td>0.007519</td>\n",
              "      <td>2.747140</td>\n",
              "      <td>0.129122</td>\n",
              "      <td>0.002014</td>\n",
              "      <td>13.366379</td>\n",
              "      <td>-0.737153</td>\n",
              "      <td>14.588851</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10000 rows × 10 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bf9188f3-139d-4e57-9d83-5575bf99ba7d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-bf9188f3-139d-4e57-9d83-5575bf99ba7d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-bf9188f3-139d-4e57-9d83-5575bf99ba7d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "      days_to_maturity  strike  volatility  mean_volatility  reversion  \\\n",
              "0                 24.0   4.250    0.002046         0.050490   0.842923   \n",
              "1                119.0   5.050    0.050203         0.048482   2.676144   \n",
              "2                 10.0  15.000    0.012146         0.009763   0.952558   \n",
              "3                235.0  14.000    0.028765         0.038385   4.946453   \n",
              "4                130.0  16.300    0.010112         0.045151   0.474549   \n",
              "...                ...     ...         ...              ...        ...   \n",
              "9995              22.0   3.975    0.003293         0.047075   0.377678   \n",
              "9996               7.0  14.000    0.022504         0.025393   0.826864   \n",
              "9997              10.0  12.400    0.002174         0.040227   0.190017   \n",
              "9998             192.0   4.450    0.044043         0.030118   4.034105   \n",
              "9999             186.0   4.950    0.050072         0.007519   2.747140   \n",
              "\n",
              "      vol_of_var      rate  Underlying_0     Rho_0  contract_price  \n",
              "0       0.631400  0.022602     58.010400 -0.320407       61.992674  \n",
              "1       0.702256  0.001616     39.998899 -0.102169       16.377944  \n",
              "2       0.693369  0.009901      5.080200 -0.394712        0.026545  \n",
              "3       0.012569  0.001688     13.283750 -0.471352        0.000000  \n",
              "4       0.218541  0.015604      4.440500 -0.101970        4.166032  \n",
              "...          ...       ...           ...       ...             ...  \n",
              "9995    0.601305  0.026390     12.496370 -0.421565       11.578677  \n",
              "9996    0.429884  0.001874     15.524250 -0.229666        3.513765  \n",
              "9997    0.502809  0.002174      4.612700 -0.243227        0.296504  \n",
              "9998    0.391629  0.025649     19.868500 -0.190983        2.275450  \n",
              "9999    0.129122  0.002014     13.366379 -0.737153       14.588851  \n",
              "\n",
              "[10000 rows x 10 columns]"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataframes_list_1 = get_dfs([1], 10000)\n",
        "dataframes_list_1[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJKoFtXosX4l"
      },
      "source": [
        "# 1 Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Define some parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print_num_epochs = 5 # print progress every print_num_epochs epochs\n",
        "\n",
        "path_to_save = os.path.join(parent_path, 'NeuralNetwork/models/')  # path to save the model\n",
        "\n",
        "patience = 10  \n",
        "\n",
        "num_epochs = 10  \n",
        "\n",
        "output_shape = (1, )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "L0dXcYN0WOp4"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "from pyexpat import model\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def model_builder_hs(dim,\n",
        "            num_layers   = 2,\n",
        "            hidden_units = [14,7],\n",
        "            output_shape = (1,),\n",
        "            activation = 'elu',\n",
        "            regularizer = None,\n",
        "            initializer = tf.keras.initializers.he_uniform(),\n",
        "            final_activation = None,\n",
        "            dropout = None,\n",
        "            batchnorm = False\n",
        "            ): \n",
        "    \"\"\"\n",
        "    Returns a model for training and testing.  \n",
        "\n",
        "    Args:\n",
        "        - dim: int, basket size\n",
        "        - num_layers: int, number of hidden layers\n",
        "        - hidden_units: list of number of hidden units in each layer\n",
        "        - output_shape: shape of the output data\n",
        "        - activation: string, activation function\n",
        "        - initializer: initializer for the weights\n",
        "        - final_activation: string, activation function of final layer\n",
        "        - dropout: list, dropout rate for each layer, default None\n",
        "        - batchnorm: bool, specifies if batch normalization is used, default False \n",
        "    \n",
        "    Output:  \n",
        "        - model: tf.keras.Model, compiled if compile is True\n",
        "    \"\"\"  \n",
        "    assert num_layers == len(hidden_units), \"Number of hidden units must match number of layers\"\n",
        "    if dropout is not None:  \n",
        "        assert num_layers == len(dropout), \"Number of dropout rates must match number of layers\"\n",
        "\n",
        "    input_shape = (7 + 2*dim,)\n",
        "\n",
        "    inputs = tf.keras.layers.Input(shape=input_shape)\n",
        "    h = tf.keras.layers.Flatten()(inputs)\n",
        "\n",
        "    for i, layer in enumerate(hidden_units):\n",
        "        h = tf.keras.layers.Dense(layer, activation=activation, kernel_regularizer= regularizer,\n",
        "                                  kernel_initializer = initializer)(h)\n",
        "        if dropout:\n",
        "            h = tf.keras.layers.Dropout(dropout[i])(h)\n",
        "        if batchnorm:\n",
        "            h = tf.keras.layers.BatchNormalization()(h)\n",
        "    if final_activation is not None:\n",
        "        outputs = tf.keras.layers.Dense(output_shape[0], activation=final_activation,\n",
        "                                        kernel_initializer = initializer)(h)\n",
        "    else:\n",
        "        outputs = tf.keras.layers.Dense(output_shape[0], \n",
        "                                        kernel_initializer = initializer)(h)\n",
        "\n",
        "    model = tf.keras.Model(inputs=inputs, outputs=outputs)  \n",
        "\n",
        "    return model   \n",
        "\n",
        "\n",
        "def tuned_model_hs(hp):\n",
        "    \"\"\"\n",
        "    Returns a compiled hyperModel for keras tuner. \n",
        "\n",
        "    \"\"\"  \n",
        "\n",
        "    # defining a set of hyperparameters for tuning and a range of values for each\n",
        "    num_layers = hp.Int('num_layers', min_value=1, max_value=5) \n",
        "    activation = hp.Choice('activation', ['elu','tanh', 'relu', 'sigmoid'])\n",
        "    learning_rate = hp.Float('learning_rate', min_value=10**(-3), max_value=0.01)\n",
        "    rate_decay = hp.Float('rate_decay', min_value=0.85, max_value=0.9995)\n",
        "    l1_reg = hp.Float('l1_regularizer', min_value=10**(-8), max_value=10**(-6.5))\n",
        "    l2_reg = hp.Float('l1_regularizer', min_value=10**(-8), max_value=10**(-6.5))\n",
        "    batchnorm = hp.Boolean(name = 'batchnorm')\n",
        "    \n",
        "    hidden_units, dropouts = [],[]\n",
        "    for i in range(num_layers):\n",
        "        hidden_unit = hp.Int(f'units_{i+1}', min_value=5, max_value=7)\n",
        "        hidden_units.append(hidden_unit)\n",
        "        dropout = hp.Float(f'dropout_{i+1}', min_value=0.0, max_value=0.5, step=0.1)\n",
        "        dropouts.append(dropout)\n",
        "\n",
        "    model = model_builder_hs(dim,\n",
        "                    num_layers = num_layers, \n",
        "                    hidden_units = hidden_units,\n",
        "                    dropout = dropouts,\n",
        "                    activation = activation,\n",
        "                    batchnorm = batchnorm,\n",
        "                    regularizer = tf.keras.regularizers.l1_l2(l1_reg,l2_reg)\n",
        "                    )\n",
        "\n",
        "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
        "        learning_rate, decay_steps = 4000, decay_rate = rate_decay, staircase = True)\n",
        "    \n",
        "    model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = lr_schedule), loss = tf.keras.losses.MeanAbsolutePercentageError(), \n",
        "                  metrics = [tf.keras.metrics.MeanSquaredError()])\n",
        "\n",
        "    return model  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "mly2un7xb5BR"
      },
      "outputs": [],
      "source": [
        "best_models_history = []\n",
        "best_hp_lists = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C7thB_r20ftY"
      },
      "source": [
        "## 1.1 Using Random Tuner:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8tuaDH-KmU5x",
        "outputId": "f2f4ff21-5b98-4299-a9e9-4dc37dc6e0c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 49s]\n",
            "val_loss: 472.1968536376953\n",
            "\n",
            "Best val_loss So Far: 472.1968536376953\n",
            "Total elapsed time: 00h 03m 02s\n",
            "Results summary\n",
            "Results in hyperparams/RandomSearch/basket_option_1\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7ffb3c5a4e50>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "activation: relu\n",
            "learning_rate: 0.0044667064282456455\n",
            "rate_decay: 0.9699629512861958\n",
            "l1_regularizer: 1.6693414021157338e-07\n",
            "batchnorm: False\n",
            "units_1: 6\n",
            "dropout_1: 0.1\n",
            "units_2: 5\n",
            "dropout_2: 0.4\n",
            "units_3: 5\n",
            "dropout_3: 0.30000000000000004\n",
            "units_4: 6\n",
            "dropout_4: 0.5\n",
            "units_5: 5\n",
            "dropout_5: 0.0\n",
            "Score: 472.1968536376953\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "activation: tanh\n",
            "learning_rate: 0.0013295413522680783\n",
            "rate_decay: 0.8519401867952281\n",
            "l1_regularizer: 2.0049884334546466e-08\n",
            "batchnorm: True\n",
            "units_1: 6\n",
            "dropout_1: 0.4\n",
            "units_2: 6\n",
            "dropout_2: 0.1\n",
            "units_3: 5\n",
            "dropout_3: 0.2\n",
            "units_4: 7\n",
            "dropout_4: 0.4\n",
            "Score: 23485.3369140625\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "activation: relu\n",
            "learning_rate: 0.007088574583965032\n",
            "rate_decay: 0.9723827606206579\n",
            "l1_regularizer: 3.8357178714531095e-08\n",
            "batchnorm: True\n",
            "units_1: 5\n",
            "dropout_1: 0.5\n",
            "units_2: 5\n",
            "dropout_2: 0.0\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.0\n",
            "Score: 44118.722900390625\n",
            "Epoch 1/100\n",
            "250/250 [==============================] - 2s 3ms/step - loss: 347766.8125 - mean_squared_error: 1173.3439 - val_loss: 129523.7188 - val_mean_squared_error: 1443.4315\n",
            "Epoch 2/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 277915.6250 - mean_squared_error: 1174.8210 - val_loss: 106397.3906 - val_mean_squared_error: 1442.6395\n",
            "Epoch 3/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 281099.5625 - mean_squared_error: 1174.6787 - val_loss: 56633.2422 - val_mean_squared_error: 1440.6284\n",
            "Epoch 4/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 325917.6250 - mean_squared_error: 1175.3309 - val_loss: 105330.4531 - val_mean_squared_error: 1441.7749\n",
            "Epoch 5/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 111382.0000 - mean_squared_error: 1174.6749 - val_loss: 126028.2969 - val_mean_squared_error: 1442.0736\n",
            "Epoch 6/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 136830.8438 - mean_squared_error: 1175.0682 - val_loss: 65719.0156 - val_mean_squared_error: 1442.3074\n",
            "Epoch 7/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 89827.6875 - mean_squared_error: 1174.9932 - val_loss: 132937.9531 - val_mean_squared_error: 1442.4402\n",
            "Epoch 8/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 72311.0703 - mean_squared_error: 1174.9675 - val_loss: 65916.1953 - val_mean_squared_error: 1442.3074\n",
            "Epoch 9/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 321786.4062 - mean_squared_error: 1175.0638 - val_loss: 184614.8594 - val_mean_squared_error: 1442.4742\n",
            "Epoch 10/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 69827.2266 - mean_squared_error: 1174.5077 - val_loss: 41526.8906 - val_mean_squared_error: 1442.3237\n",
            "Epoch 11/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 68255.5625 - mean_squared_error: 1174.7329 - val_loss: 73916.2578 - val_mean_squared_error: 1442.3024\n",
            "Epoch 12/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61493.1875 - mean_squared_error: 1174.0134 - val_loss: 57898.8867 - val_mean_squared_error: 1442.3130\n",
            "Epoch 13/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 68956.7891 - mean_squared_error: 1174.7552 - val_loss: 75247.9062 - val_mean_squared_error: 1442.3013\n",
            "Epoch 14/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 87432.4922 - mean_squared_error: 1174.7288 - val_loss: 67203.4219 - val_mean_squared_error: 1442.3068\n",
            "Epoch 15/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 91452.0938 - mean_squared_error: 1175.0758 - val_loss: 75604.4219 - val_mean_squared_error: 1442.3011\n",
            "Epoch 16/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 77824.9062 - mean_squared_error: 1174.8312 - val_loss: 35834.7656 - val_mean_squared_error: 1442.3752\n",
            "Epoch 17/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 86410.6406 - mean_squared_error: 1175.1534 - val_loss: 44816.3672 - val_mean_squared_error: 1442.3219\n",
            "Epoch 18/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 69894.2344 - mean_squared_error: 1175.1619 - val_loss: 43977.8047 - val_mean_squared_error: 1442.3223\n",
            "Epoch 19/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 48073.5078 - mean_squared_error: 1174.8573 - val_loss: 68478.6562 - val_mean_squared_error: 1442.3971\n",
            "Epoch 20/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 66047.1719 - mean_squared_error: 1174.9475 - val_loss: 32339.8477 - val_mean_squared_error: 1442.3730\n",
            "Epoch 21/100\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 73744.2266 - mean_squared_error: 1174.9762 - val_loss: 71274.0625 - val_mean_squared_error: 1442.3992\n",
            "Epoch 22/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61173.5000 - mean_squared_error: 1174.9802 - val_loss: 54872.3477 - val_mean_squared_error: 1442.3878\n",
            "Epoch 23/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 72477.0234 - mean_squared_error: 1175.0142 - val_loss: 40806.0781 - val_mean_squared_error: 1442.3785\n",
            "Epoch 24/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 72755.1484 - mean_squared_error: 1174.6553 - val_loss: 84838.9297 - val_mean_squared_error: 1442.2952\n",
            "Epoch 25/100\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 44135.4102 - mean_squared_error: 1175.0050 - val_loss: 47000.6328 - val_mean_squared_error: 1442.3199\n",
            "Epoch 26/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 58979.8125 - mean_squared_error: 1174.5312 - val_loss: 133825.4688 - val_mean_squared_error: 1442.4403\n",
            "Epoch 27/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 64429.4648 - mean_squared_error: 1174.7596 - val_loss: 116251.4922 - val_mean_squared_error: 1442.2742\n",
            "Epoch 28/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 76137.8516 - mean_squared_error: 1174.8817 - val_loss: 132066.0625 - val_mean_squared_error: 1442.4395\n",
            "Epoch 29/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 69358.4922 - mean_squared_error: 1175.0658 - val_loss: 68885.8750 - val_mean_squared_error: 1442.3975\n",
            "Epoch 30/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 76677.8828 - mean_squared_error: 1174.8589 - val_loss: 60153.2188 - val_mean_squared_error: 1442.3915\n",
            "Epoch 31/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 68160.4766 - mean_squared_error: 1174.8191 - val_loss: 104486.5312 - val_mean_squared_error: 1442.4209\n",
            "Epoch 32/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 63625.4141 - mean_squared_error: 1174.8324 - val_loss: 136552.8906 - val_mean_squared_error: 1442.3080\n",
            "Epoch 33/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61348.9609 - mean_squared_error: 1174.8466 - val_loss: 123154.2891 - val_mean_squared_error: 1441.7319\n",
            "Epoch 34/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61141.4375 - mean_squared_error: 1174.9976 - val_loss: 130285.7578 - val_mean_squared_error: 1441.6960\n",
            "Epoch 35/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 63222.6133 - mean_squared_error: 1174.7988 - val_loss: 119384.0547 - val_mean_squared_error: 1441.6567\n",
            "Epoch 36/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 63581.8086 - mean_squared_error: 1174.7535 - val_loss: 56571.2773 - val_mean_squared_error: 1441.7365\n",
            "Epoch 37/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 66578.9062 - mean_squared_error: 1174.8528 - val_loss: 73383.6016 - val_mean_squared_error: 1441.7876\n",
            "Epoch 38/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 75332.0078 - mean_squared_error: 1175.0238 - val_loss: 34475.6914 - val_mean_squared_error: 1441.9548\n",
            "Epoch 39/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 83680.4219 - mean_squared_error: 1174.8937 - val_loss: 43622.8984 - val_mean_squared_error: 1441.7900\n",
            "Epoch 40/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 67711.2031 - mean_squared_error: 1174.6813 - val_loss: 42825.0117 - val_mean_squared_error: 1441.8846\n",
            "Epoch 41/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 46585.5898 - mean_squared_error: 1174.7301 - val_loss: 66143.7031 - val_mean_squared_error: 1441.8652\n",
            "Epoch 42/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 64097.0469 - mean_squared_error: 1174.7285 - val_loss: 31099.7207 - val_mean_squared_error: 1441.9984\n",
            "Epoch 43/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 60546.6680 - mean_squared_error: 1174.7336 - val_loss: 68476.1250 - val_mean_squared_error: 1441.8040\n",
            "Epoch 44/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57512.5664 - mean_squared_error: 1174.6699 - val_loss: 53052.7812 - val_mean_squared_error: 1441.5879\n",
            "Epoch 45/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 56790.4414 - mean_squared_error: 1174.8082 - val_loss: 112725.2266 - val_mean_squared_error: 1441.3088\n",
            "Epoch 46/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 69728.1328 - mean_squared_error: 1174.5580 - val_loss: 56724.2383 - val_mean_squared_error: 1441.9828\n",
            "Epoch 47/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 66108.8516 - mean_squared_error: 1175.0690 - val_loss: 101327.2812 - val_mean_squared_error: 1441.9530\n",
            "Epoch 48/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61707.7578 - mean_squared_error: 1174.9612 - val_loss: 133328.7812 - val_mean_squared_error: 1441.9310\n",
            "Epoch 49/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 59515.3438 - mean_squared_error: 1174.7908 - val_loss: 117624.0547 - val_mean_squared_error: 1442.0978\n",
            "Epoch 50/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 49934.3086 - mean_squared_error: 1174.8927 - val_loss: 142075.2031 - val_mean_squared_error: 1442.1141\n",
            "Epoch 51/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 63491.0625 - mean_squared_error: 1174.9301 - val_loss: 19358.2266 - val_mean_squared_error: 1442.0321\n",
            "Epoch 52/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 85152.7969 - mean_squared_error: 1174.7766 - val_loss: 140023.2812 - val_mean_squared_error: 1441.9257\n",
            "Epoch 53/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61113.3477 - mean_squared_error: 1174.8613 - val_loss: 3321.0859 - val_mean_squared_error: 1442.0208\n",
            "Epoch 54/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 84417.4609 - mean_squared_error: 1175.0450 - val_loss: 138363.8125 - val_mean_squared_error: 1441.9258\n",
            "Epoch 55/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 61107.9062 - mean_squared_error: 1174.6177 - val_loss: 4980.4424 - val_mean_squared_error: 1442.0205\n",
            "Epoch 56/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 84422.6250 - mean_squared_error: 1174.9963 - val_loss: 136718.5156 - val_mean_squared_error: 1441.9263\n",
            "Epoch 57/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57894.2305 - mean_squared_error: 1174.8086 - val_loss: 115254.3906 - val_mean_squared_error: 1441.9417\n",
            "Epoch 58/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 58056.8398 - mean_squared_error: 1174.8149 - val_loss: 42479.3906 - val_mean_squared_error: 1441.9901\n",
            "Epoch 59/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 73956.3516 - mean_squared_error: 1175.0422 - val_loss: 115640.9375 - val_mean_squared_error: 1442.0964\n",
            "Epoch 60/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 56337.7109 - mean_squared_error: 1175.0059 - val_loss: 40651.4336 - val_mean_squared_error: 1442.0458\n",
            "Epoch 61/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57095.3945 - mean_squared_error: 1174.8735 - val_loss: 112359.9062 - val_mean_squared_error: 1441.9442\n",
            "Epoch 62/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 59823.7266 - mean_squared_error: 1174.9575 - val_loss: 53265.9219 - val_mean_squared_error: 1441.9833\n",
            "Epoch 63/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 62643.5625 - mean_squared_error: 1174.7917 - val_loss: 69083.4062 - val_mean_squared_error: 1441.9733\n",
            "Epoch 64/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 64182.3750 - mean_squared_error: 1174.8699 - val_loss: 32365.5723 - val_mean_squared_error: 1442.0397\n",
            "Epoch 65/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 78733.3750 - mean_squared_error: 1175.1553 - val_loss: 41135.3906 - val_mean_squared_error: 1441.9934\n",
            "Epoch 66/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 63709.9297 - mean_squared_error: 1175.0098 - val_loss: 40384.7422 - val_mean_squared_error: 1441.9961\n",
            "Epoch 67/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 56953.6172 - mean_squared_error: 1174.9104 - val_loss: 113446.6562 - val_mean_squared_error: 1441.9438\n",
            "Epoch 68/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 49504.8750 - mean_squared_error: 1174.8739 - val_loss: 12862.2783 - val_mean_squared_error: 1442.0273\n",
            "Epoch 69/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 47904.3047 - mean_squared_error: 1174.9048 - val_loss: 25125.7305 - val_mean_squared_error: 1442.0354\n",
            "Epoch 70/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 60052.3164 - mean_squared_error: 1174.8007 - val_loss: 60546.7305 - val_mean_squared_error: 1442.0566\n",
            "Epoch 71/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 66395.9922 - mean_squared_error: 1174.8656 - val_loss: 10041.9502 - val_mean_squared_error: 1442.0184\n",
            "Epoch 72/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 50694.5000 - mean_squared_error: 1174.8745 - val_loss: 128231.1094 - val_mean_squared_error: 1442.0881\n",
            "Epoch 73/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57719.5430 - mean_squared_error: 1174.7997 - val_loss: 116109.6719 - val_mean_squared_error: 1441.9177\n",
            "Epoch 74/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57526.3164 - mean_squared_error: 1174.7502 - val_loss: 122820.1172 - val_mean_squared_error: 1441.9084\n",
            "Epoch 75/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 59486.0430 - mean_squared_error: 1174.9211 - val_loss: 112564.7578 - val_mean_squared_error: 1441.9143\n",
            "Epoch 76/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 55800.6289 - mean_squared_error: 1174.8676 - val_loss: 119155.6094 - val_mean_squared_error: 1441.9055\n",
            "Epoch 77/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57702.0977 - mean_squared_error: 1174.7593 - val_loss: 109208.2891 - val_mean_squared_error: 1441.9058\n",
            "Epoch 78/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 58030.2812 - mean_squared_error: 1174.7988 - val_loss: 51889.2930 - val_mean_squared_error: 1441.9277\n",
            "Epoch 79/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 62097.9922 - mean_squared_error: 1174.8442 - val_loss: 106736.5547 - val_mean_squared_error: 1442.0090\n",
            "Epoch 80/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 58016.3281 - mean_squared_error: 1174.5806 - val_loss: 50800.2773 - val_mean_squared_error: 1441.9552\n",
            "Epoch 81/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 64389.3945 - mean_squared_error: 1174.5465 - val_loss: 9700.3457 - val_mean_squared_error: 1441.9034\n",
            "Epoch 82/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 49174.7891 - mean_squared_error: 1174.5315 - val_loss: 124329.8516 - val_mean_squared_error: 1441.9564\n",
            "Epoch 83/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 55988.2578 - mean_squared_error: 1174.6400 - val_loss: 112678.2891 - val_mean_squared_error: 1441.7810\n",
            "Epoch 84/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 55801.0000 - mean_squared_error: 1174.8143 - val_loss: 119197.7656 - val_mean_squared_error: 1441.7849\n",
            "Epoch 85/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 57702.2852 - mean_squared_error: 1174.5179 - val_loss: 109250.3906 - val_mean_squared_error: 1441.7629\n",
            "Epoch 86/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 58030.4297 - mean_squared_error: 1174.8352 - val_loss: 51931.2578 - val_mean_squared_error: 1441.7720\n",
            "Epoch 87/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 62098.1211 - mean_squared_error: 1174.2146 - val_loss: 106694.7266 - val_mean_squared_error: 1441.8633\n",
            "Epoch 88/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 58016.2539 - mean_squared_error: 1174.7974 - val_loss: 50758.4570 - val_mean_squared_error: 1441.7738\n",
            "Epoch 89/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 64389.5195 - mean_squared_error: 1174.7744 - val_loss: 9658.5234 - val_mean_squared_error: 1441.7025\n",
            "Epoch 90/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 46560.5547 - mean_squared_error: 1174.5240 - val_loss: 12911.6865 - val_mean_squared_error: 1441.7207\n",
            "Epoch 91/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 49206.2578 - mean_squared_error: 1174.8339 - val_loss: 104030.5625 - val_mean_squared_error: 1441.7021\n",
            "Epoch 92/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 56281.1328 - mean_squared_error: 1174.5685 - val_loss: 48824.8281 - val_mean_squared_error: 1441.5852\n",
            "Epoch 93/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 62459.4141 - mean_squared_error: 1174.8816 - val_loss: 8959.2598 - val_mean_squared_error: 1441.5089\n",
            "Epoch 94/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 45163.6328 - mean_squared_error: 1174.4679 - val_loss: 12114.7402 - val_mean_squared_error: 1441.3617\n",
            "Epoch 95/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 243507.5000 - mean_squared_error: 1176.0625 - val_loss: 31573.4727 - val_mean_squared_error: 1442.3302\n",
            "Epoch 96/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 52008.4727 - mean_squared_error: 1175.6273 - val_loss: 67657.4453 - val_mean_squared_error: 1442.3065\n",
            "Epoch 97/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 43403.3867 - mean_squared_error: 1175.7166 - val_loss: 58073.3633 - val_mean_squared_error: 1442.3899\n",
            "Epoch 98/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 56748.4336 - mean_squared_error: 1175.7015 - val_loss: 27053.7480 - val_mean_squared_error: 1442.3693\n",
            "Epoch 99/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 53603.1289 - mean_squared_error: 1175.3823 - val_loss: 60068.6758 - val_mean_squared_error: 1442.3915\n",
            "Epoch 100/100\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 151767.4062 - mean_squared_error: 1175.0896 - val_loss: 99992.4531 - val_mean_squared_error: 1442.4181\n"
          ]
        }
      ],
      "source": [
        "dims = [1]\n",
        "for i, dim in enumerate(dims):\n",
        "    train_ds, valid_ds, test_ds = pipeline1(dataframes_list_1[i].to_numpy(), scaling=False)\n",
        "\n",
        "    train_copy, valid_copy, test_copy = pipeline1(dataframes_list_1[i].to_numpy(), prefetch=False)\n",
        "\n",
        "\n",
        "    print(f\"Start training for basket option with size {dim}:\")\n",
        "\n",
        "    random_tuner = kt.RandomSearch(\n",
        "    hypermodel=tuned_model_hs, # the hypermodel to tune # can be tuneLR or tuneLayer\n",
        "    objective=\"val_loss\", # the objective to optimize\n",
        "    max_trials=3, # the maximum number of trials to run\n",
        "    executions_per_trial=2, # the number of models generated on each trial\n",
        "    overwrite=True, # whether to overwrite previous trials\n",
        "    directory=\"hyperparams/RandomSearch\", # the directory to save the trials\n",
        "    project_name=f\"basket_option_{dim}\", # the name of the project\n",
        "    )  \n",
        "\n",
        "    random_tuner.search(train_ds, epochs = 30, validation_data = valid_ds)    \n",
        "\n",
        "    best_hp_lists.append(random_tuner.get_best_hyperparameters(1)[0])\n",
        "    models = random_tuner.get_best_models(num_models=1)\n",
        "    best_model = models[0]\n",
        "\n",
        "    random_tuner.results_summary()\n",
        "    \n",
        "    begin_train = time.time()\n",
        "    history = best_model.fit(train_ds, epochs = 100, validation_data = valid_ds)\n",
        "    end_train = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzx5yenRvjnO",
        "outputId": "0d53ac3e-28ec-4920-8f72-b3beb9428a9a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 70.80026173591614 seconds\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training time: {end_train - begin_train} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-XlTAUD0bZZ"
      },
      "source": [
        "## 1.2 Using Hyperband Tuner:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rmx0nvo0vuG",
        "outputId": "8db32712-c36b-4447-a8cd-013a4beb84af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 30 Complete [00h 00m 12s]\n",
            "val_loss: 6234.1611328125\n",
            "\n",
            "Best val_loss So Far: 600.0084228515625\n",
            "Total elapsed time: 00h 02m 41s\n",
            "Results summary\n",
            "Results in hyperparams/HyperbandSearch/basket_option_1\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7ffb3c1f4dd0>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "activation: relu\n",
            "learning_rate: 0.0011543003071872165\n",
            "rate_decay: 0.9571126414036437\n",
            "l1_regularizer: 2.613237022535335e-07\n",
            "batchnorm: False\n",
            "units_1: 5\n",
            "dropout_1: 0.5\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "units_2: 5\n",
            "dropout_2: 0.0\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.0\n",
            "units_5: 5\n",
            "dropout_5: 0.0\n",
            "Score: 600.0084228515625\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "activation: elu\n",
            "learning_rate: 0.005835978452452756\n",
            "rate_decay: 0.8652186237061492\n",
            "l1_regularizer: 7.989614041811944e-08\n",
            "batchnorm: False\n",
            "units_1: 6\n",
            "dropout_1: 0.0\n",
            "units_2: 6\n",
            "dropout_2: 0.4\n",
            "units_3: 6\n",
            "dropout_3: 0.0\n",
            "units_4: 7\n",
            "dropout_4: 0.0\n",
            "units_5: 7\n",
            "dropout_5: 0.30000000000000004\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 6234.1611328125\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "activation: relu\n",
            "learning_rate: 0.0093908492397318\n",
            "rate_decay: 0.9558350700434024\n",
            "l1_regularizer: 4.0626257561862645e-08\n",
            "batchnorm: False\n",
            "units_1: 7\n",
            "dropout_1: 0.1\n",
            "units_2: 7\n",
            "dropout_2: 0.5\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.4\n",
            "units_5: 6\n",
            "dropout_5: 0.4\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 2\n",
            "tuner/round: 2\n",
            "tuner/trial_id: 0014\n",
            "Score: 13029.0380859375\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "activation: relu\n",
            "learning_rate: 0.0011543003071872165\n",
            "rate_decay: 0.9571126414036437\n",
            "l1_regularizer: 2.613237022535335e-07\n",
            "batchnorm: False\n",
            "units_1: 5\n",
            "dropout_1: 0.5\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "units_2: 5\n",
            "dropout_2: 0.0\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.0\n",
            "units_5: 5\n",
            "dropout_5: 0.0\n",
            "tuner/trial_id: 0000\n",
            "Score: 14796.7685546875\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "activation: tanh\n",
            "learning_rate: 0.008093643952687407\n",
            "rate_decay: 0.8708313408125558\n",
            "l1_regularizer: 7.636246335130573e-08\n",
            "batchnorm: True\n",
            "units_1: 6\n",
            "dropout_1: 0.1\n",
            "units_2: 6\n",
            "dropout_2: 0.4\n",
            "units_3: 7\n",
            "dropout_3: 0.4\n",
            "units_4: 6\n",
            "dropout_4: 0.4\n",
            "units_5: 5\n",
            "dropout_5: 0.1\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 21877.90625\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "activation: relu\n",
            "learning_rate: 0.0093908492397318\n",
            "rate_decay: 0.9558350700434024\n",
            "l1_regularizer: 4.0626257561862645e-08\n",
            "batchnorm: False\n",
            "units_1: 7\n",
            "dropout_1: 0.1\n",
            "units_2: 7\n",
            "dropout_2: 0.5\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.4\n",
            "units_5: 6\n",
            "dropout_5: 0.4\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 53041.63671875\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "activation: elu\n",
            "learning_rate: 0.006402517797732144\n",
            "rate_decay: 0.985876484896921\n",
            "l1_regularizer: 1.4266660070570587e-07\n",
            "batchnorm: True\n",
            "units_1: 6\n",
            "dropout_1: 0.5\n",
            "units_2: 6\n",
            "dropout_2: 0.4\n",
            "units_3: 6\n",
            "dropout_3: 0.1\n",
            "units_4: 6\n",
            "dropout_4: 0.2\n",
            "units_5: 6\n",
            "dropout_5: 0.30000000000000004\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 4\n",
            "tuner/bracket: 1\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0018\n",
            "Score: 58331.12109375\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "activation: elu\n",
            "learning_rate: 0.007027203285715031\n",
            "rate_decay: 0.9375806004132138\n",
            "l1_regularizer: 2.0046000220695772e-07\n",
            "batchnorm: True\n",
            "units_1: 7\n",
            "dropout_1: 0.4\n",
            "units_2: 5\n",
            "dropout_2: 0.4\n",
            "units_3: 6\n",
            "dropout_3: 0.2\n",
            "units_4: 5\n",
            "dropout_4: 0.4\n",
            "units_5: 5\n",
            "dropout_5: 0.4\n",
            "tuner/epochs: 2\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 2\n",
            "tuner/round: 0\n",
            "Score: 70833.75\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 3\n",
            "activation: tanh\n",
            "learning_rate: 0.007753134065170282\n",
            "rate_decay: 0.8958443224852808\n",
            "l1_regularizer: 1.7850324187974308e-07\n",
            "batchnorm: True\n",
            "units_1: 7\n",
            "dropout_1: 0.0\n",
            "units_2: 5\n",
            "dropout_2: 0.4\n",
            "units_3: 7\n",
            "dropout_3: 0.5\n",
            "units_4: 6\n",
            "dropout_4: 0.4\n",
            "units_5: 5\n",
            "dropout_5: 0.1\n",
            "tuner/epochs: 10\n",
            "tuner/initial_epoch: 0\n",
            "tuner/bracket: 0\n",
            "tuner/round: 0\n",
            "Score: 73767.3828125\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 4\n",
            "activation: relu\n",
            "learning_rate: 0.0093908492397318\n",
            "rate_decay: 0.9558350700434024\n",
            "l1_regularizer: 4.0626257561862645e-08\n",
            "batchnorm: False\n",
            "units_1: 7\n",
            "dropout_1: 0.1\n",
            "units_2: 7\n",
            "dropout_2: 0.5\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.4\n",
            "units_5: 6\n",
            "dropout_5: 0.4\n",
            "tuner/epochs: 4\n",
            "tuner/initial_epoch: 2\n",
            "tuner/bracket: 2\n",
            "tuner/round: 1\n",
            "tuner/trial_id: 0001\n",
            "Score: 84154.46875\n",
            "Epoch 1/30\n",
            "250/250 [==============================] - 2s 3ms/step - loss: 6933057.5000 - mean_squared_error: 1382.2051 - val_loss: 32227.9141 - val_mean_squared_error: 705.0084\n",
            "Epoch 2/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 5270983.5000 - mean_squared_error: 1382.0774 - val_loss: 20878.8555 - val_mean_squared_error: 704.9547\n",
            "Epoch 3/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 4343165.0000 - mean_squared_error: 1382.0328 - val_loss: 21322.7891 - val_mean_squared_error: 704.9478\n",
            "Epoch 4/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 1031195.0625 - mean_squared_error: 1382.0266 - val_loss: 7910.0020 - val_mean_squared_error: 704.9575\n",
            "Epoch 5/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 2200943.5000 - mean_squared_error: 1382.0315 - val_loss: 6647.2500 - val_mean_squared_error: 704.9680\n",
            "Epoch 6/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 2095430.0000 - mean_squared_error: 1382.0020 - val_loss: 15423.5000 - val_mean_squared_error: 704.9526\n",
            "Epoch 7/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1907428.3750 - mean_squared_error: 1382.0281 - val_loss: 7375.5176 - val_mean_squared_error: 704.9592\n",
            "Epoch 8/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 1320714.6250 - mean_squared_error: 1382.0225 - val_loss: 2121.5210 - val_mean_squared_error: 704.9673\n",
            "Epoch 9/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 740119.0625 - mean_squared_error: 1382.0051 - val_loss: 24158.1035 - val_mean_squared_error: 704.9485\n",
            "Epoch 10/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 1600608.0000 - mean_squared_error: 1382.0035 - val_loss: 7210.1577 - val_mean_squared_error: 704.9608\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 1363645.8750 - mean_squared_error: 1381.9686 - val_loss: 4078.2849 - val_mean_squared_error: 704.9706\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 1626513.5000 - mean_squared_error: 1382.0175 - val_loss: 12764.6484 - val_mean_squared_error: 704.9775\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 432639.5000 - mean_squared_error: 1382.0212 - val_loss: 2533.0251 - val_mean_squared_error: 704.9662\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 1754033.1250 - mean_squared_error: 1382.0260 - val_loss: 1419.8770 - val_mean_squared_error: 704.9636\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 396546.0625 - mean_squared_error: 1382.0084 - val_loss: 20643.3848 - val_mean_squared_error: 704.9493\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 761991.4375 - mean_squared_error: 1382.0118 - val_loss: 6540.3003 - val_mean_squared_error: 704.9598\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 702300.1875 - mean_squared_error: 1382.0121 - val_loss: 24057.2109 - val_mean_squared_error: 704.9817\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 491651.2500 - mean_squared_error: 1382.0133 - val_loss: 8589.2510 - val_mean_squared_error: 704.9581\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 141305.7500 - mean_squared_error: 1382.0109 - val_loss: 24017.3887 - val_mean_squared_error: 704.9816\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 59215.1953 - mean_squared_error: 1382.0135 - val_loss: 22749.6680 - val_mean_squared_error: 704.9481\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 379753.5938 - mean_squared_error: 1382.0125 - val_loss: 7534.9258 - val_mean_squared_error: 704.9587\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 46232.3711 - mean_squared_error: 1382.0142 - val_loss: 14293.1982 - val_mean_squared_error: 704.9745\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 23944.3223 - mean_squared_error: 1382.0142 - val_loss: 176.6058 - val_mean_squared_error: 704.9642\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 1s 4ms/step - loss: 36288.6055 - mean_squared_error: 1382.0129 - val_loss: 11885.5439 - val_mean_squared_error: 704.9727\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 21538.3320 - mean_squared_error: 1382.0129 - val_loss: 19449.2852 - val_mean_squared_error: 704.9783\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 24200.8555 - mean_squared_error: 1382.0117 - val_loss: 4154.4619 - val_mean_squared_error: 704.9613\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 154521.4062 - mean_squared_error: 1382.0134 - val_loss: 4798.9268 - val_mean_squared_error: 704.9666\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 16454.2422 - mean_squared_error: 1382.0088 - val_loss: 4151.9312 - val_mean_squared_error: 704.9604\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 41412.6797 - mean_squared_error: 1382.0092 - val_loss: 5665.6851 - val_mean_squared_error: 704.9673\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 13452.3018 - mean_squared_error: 1382.0090 - val_loss: 19169.0352 - val_mean_squared_error: 704.9772\n"
          ]
        }
      ],
      "source": [
        "dims = [1]\n",
        "for i, dim in enumerate(dims):\n",
        "    train_ds, valid_ds, test_ds = pipeline1(dataframes_list_1[i].to_numpy(), scaling=False)\n",
        "\n",
        "    train_copy, valid_copy, test_copy = pipeline1(dataframes_list_1[i].to_numpy(), prefetch=False)\n",
        "\n",
        "\n",
        "    print(f\"Start training for basket option with size {dim}:\")\n",
        "\n",
        "    Hyperband_tuner = kt.Hyperband(\n",
        "    hypermodel=tuned_model_hs, # the hypermodel to tune # can be tuneLR or tuneLayer\n",
        "    objective=\"val_loss\", # the objective to optimize\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    directory=\"hyperparams/HyperbandSearch\", # the directory to save the trials\n",
        "    project_name=f\"basket_option_{dim}\", # the name of the project\n",
        "    )  \n",
        "\n",
        "    Hyperband_tuner.search(train_ds, epochs = 30, validation_data = valid_ds)    \n",
        "\n",
        "    best_hp_lists.append(Hyperband_tuner.get_best_hyperparameters(1)[0])\n",
        "    models = Hyperband_tuner.get_best_models(num_models=1)\n",
        "    best_model = models[0]\n",
        "\n",
        "    Hyperband_tuner.results_summary()\n",
        "    \n",
        "    begin_train = time.time()\n",
        "    history = best_model.fit(train_ds, epochs = 30, validation_data = valid_ds)\n",
        "    end_train = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QkzA7PCl2Ow4",
        "outputId": "0afac362-7a30-4ad1-b1a4-3eb0bcf7d1f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 26.55580735206604 seconds\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training time: {end_train - begin_train} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_g0x48b2oVu"
      },
      "source": [
        "## 1.3 Using Bayesian Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmoM_mDx2SPH",
        "outputId": "3d98cb86-529b-4110-8b6a-821e76c37ccb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 3 Complete [00h 00m 36s]\n",
            "val_loss: 114491.828125\n",
            "\n",
            "Best val_loss So Far: 13906.3056640625\n",
            "Total elapsed time: 00h 01m 36s\n",
            "Results summary\n",
            "Results in hyperparams/BayesianOptimization/basket_option_1\n",
            "Showing 10 best trials\n",
            "<keras_tuner.engine.objective.Objective object at 0x7ffb3ad37290>\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 2\n",
            "activation: sigmoid\n",
            "learning_rate: 0.008117221165611607\n",
            "rate_decay: 0.8955589475710379\n",
            "l1_regularizer: 2.1512359334152384e-07\n",
            "batchnorm: True\n",
            "units_1: 6\n",
            "dropout_1: 0.2\n",
            "units_2: 5\n",
            "dropout_2: 0.0\n",
            "Score: 13906.3056640625\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 1\n",
            "activation: sigmoid\n",
            "learning_rate: 0.008447677757371937\n",
            "rate_decay: 0.9058101961212783\n",
            "l1_regularizer: 6.642022198638799e-08\n",
            "batchnorm: False\n",
            "units_1: 6\n",
            "dropout_1: 0.30000000000000004\n",
            "Score: 16483.38671875\n",
            "Trial summary\n",
            "Hyperparameters:\n",
            "num_layers: 5\n",
            "activation: sigmoid\n",
            "learning_rate: 0.0064758658584953025\n",
            "rate_decay: 0.85\n",
            "l1_regularizer: 3.162277660168379e-07\n",
            "batchnorm: True\n",
            "units_1: 6\n",
            "dropout_1: 0.0\n",
            "units_2: 5\n",
            "dropout_2: 0.0\n",
            "units_3: 5\n",
            "dropout_3: 0.0\n",
            "units_4: 5\n",
            "dropout_4: 0.0\n",
            "units_5: 5\n",
            "dropout_5: 0.0\n",
            "Score: 114491.828125\n",
            "Epoch 1/30\n",
            "250/250 [==============================] - 2s 3ms/step - loss: 199618.3594 - mean_squared_error: 1248.2450 - val_loss: 265387.0625 - val_mean_squared_error: 1193.0411\n",
            "Epoch 2/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 160507.2031 - mean_squared_error: 1248.2325 - val_loss: 203557.0625 - val_mean_squared_error: 1193.4935\n",
            "Epoch 3/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 165680.9375 - mean_squared_error: 1248.2666 - val_loss: 101832.0625 - val_mean_squared_error: 1193.2332\n",
            "Epoch 4/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 148797.8125 - mean_squared_error: 1248.3181 - val_loss: 278480.2812 - val_mean_squared_error: 1193.1685\n",
            "Epoch 5/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 127093.9062 - mean_squared_error: 1248.3611 - val_loss: 178909.5469 - val_mean_squared_error: 1193.1940\n",
            "Epoch 6/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 140765.5312 - mean_squared_error: 1248.3313 - val_loss: 87365.1094 - val_mean_squared_error: 1193.4014\n",
            "Epoch 7/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 106108.8672 - mean_squared_error: 1248.3462 - val_loss: 77197.1094 - val_mean_squared_error: 1193.3245\n",
            "Epoch 8/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 75828.9844 - mean_squared_error: 1248.3784 - val_loss: 57011.6602 - val_mean_squared_error: 1193.3970\n",
            "Epoch 9/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 80964.6016 - mean_squared_error: 1248.2996 - val_loss: 21367.5801 - val_mean_squared_error: 1193.4137\n",
            "Epoch 10/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 99445.4219 - mean_squared_error: 1248.3081 - val_loss: 11295.1035 - val_mean_squared_error: 1193.5463\n",
            "Epoch 11/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 95960.6797 - mean_squared_error: 1248.2841 - val_loss: 19995.6035 - val_mean_squared_error: 1192.8391\n",
            "Epoch 12/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 118650.4844 - mean_squared_error: 1248.1807 - val_loss: 134294.9062 - val_mean_squared_error: 1192.9811\n",
            "Epoch 13/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 112750.3516 - mean_squared_error: 1248.3936 - val_loss: 169965.8594 - val_mean_squared_error: 1193.3696\n",
            "Epoch 14/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 114278.8438 - mean_squared_error: 1248.3186 - val_loss: 38064.6602 - val_mean_squared_error: 1193.3486\n",
            "Epoch 15/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 121744.0000 - mean_squared_error: 1248.4362 - val_loss: 43522.1836 - val_mean_squared_error: 1192.7805\n",
            "Epoch 16/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 102157.6797 - mean_squared_error: 1248.4252 - val_loss: 168775.4688 - val_mean_squared_error: 1193.6317\n",
            "Epoch 17/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 104723.4062 - mean_squared_error: 1248.2990 - val_loss: 81502.1719 - val_mean_squared_error: 1193.3000\n",
            "Epoch 18/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 80865.7031 - mean_squared_error: 1248.5793 - val_loss: 84069.9688 - val_mean_squared_error: 1193.4579\n",
            "Epoch 19/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 99546.7812 - mean_squared_error: 1248.2277 - val_loss: 178643.5469 - val_mean_squared_error: 1193.1462\n",
            "Epoch 20/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 115515.6406 - mean_squared_error: 1248.3136 - val_loss: 89304.1172 - val_mean_squared_error: 1193.4318\n",
            "Epoch 21/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 107836.2500 - mean_squared_error: 1248.2539 - val_loss: 122938.8516 - val_mean_squared_error: 1193.4789\n",
            "Epoch 22/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 102533.6406 - mean_squared_error: 1248.4966 - val_loss: 19240.2578 - val_mean_squared_error: 1193.3689\n",
            "Epoch 23/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 78104.5312 - mean_squared_error: 1248.2843 - val_loss: 71254.3281 - val_mean_squared_error: 1193.3207\n",
            "Epoch 24/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 144828.5000 - mean_squared_error: 1248.4739 - val_loss: 79620.9453 - val_mean_squared_error: 1193.6223\n",
            "Epoch 25/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 103440.0469 - mean_squared_error: 1248.4332 - val_loss: 215252.5000 - val_mean_squared_error: 1192.9797\n",
            "Epoch 26/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 123808.5234 - mean_squared_error: 1248.3041 - val_loss: 22281.1699 - val_mean_squared_error: 1193.3926\n",
            "Epoch 27/30\n",
            "250/250 [==============================] - 1s 2ms/step - loss: 73765.5547 - mean_squared_error: 1248.2827 - val_loss: 96347.9297 - val_mean_squared_error: 1193.4919\n",
            "Epoch 28/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 104801.2734 - mean_squared_error: 1248.4108 - val_loss: 8341.2305 - val_mean_squared_error: 1193.3383\n",
            "Epoch 29/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 97707.0703 - mean_squared_error: 1248.4445 - val_loss: 56831.3711 - val_mean_squared_error: 1193.4120\n",
            "Epoch 30/30\n",
            "250/250 [==============================] - 1s 3ms/step - loss: 94197.5859 - mean_squared_error: 1248.3634 - val_loss: 142662.2969 - val_mean_squared_error: 1193.4531\n"
          ]
        }
      ],
      "source": [
        "dims = [1]\n",
        "for i, dim in enumerate(dims):\n",
        "    train_ds, valid_ds, test_ds = pipeline1(dataframes_list_1[i].to_numpy(), scaling=False)\n",
        "\n",
        "    train_copy, valid_copy, test_copy = pipeline1(dataframes_list_1[i].to_numpy(), prefetch=False)\n",
        "\n",
        "\n",
        "    print(f\"Start training for basket option with size {dim}:\")\n",
        "\n",
        "    bayes_tuner = kt.BayesianOptimization(\n",
        "    hypermodel=tuned_model_hs, # the hypermodel to tune # can be tuneLR or tuneLayer\n",
        "    objective=\"val_loss\", # the objective to optimize\n",
        "    max_trials=3,\n",
        "    directory=\"hyperparams/BayesianOptimization\", # the directory to save the trials\n",
        "    project_name=f\"basket_option_{dim}\", # the name of the project\n",
        "    )  \n",
        "\n",
        "    bayes_tuner.search(train_ds, epochs = 30, validation_data = valid_ds)    \n",
        "\n",
        "    best_hp_lists.append(bayes_tuner.get_best_hyperparameters(1)[0])\n",
        "    models = bayes_tuner.get_best_models(num_models=1)\n",
        "    best_model = models[0]\n",
        "\n",
        "    bayes_tuner.results_summary()\n",
        "    \n",
        "    begin_train = time.time()\n",
        "    history = best_model.fit(train_ds, epochs = 30, validation_data = valid_ds)\n",
        "    end_train = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3h5EiVq3IFh",
        "outputId": "08eea799-23da-4daa-e148-e40e8609e6de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training time: 34.867894411087036 seconds\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training time: {end_train - begin_train} seconds\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
