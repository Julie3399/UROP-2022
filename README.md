# Option Pricing with Machine Learning Methods  

*This is a repository for UROP summer 2022.*  
&nbsp;  

<p align="center">
<img src="./assets/IMG_1727.jpeg">
</p>
<p align="center">
<em>Image generated by Stable Diffusion </em>
</p>


## Background   

Option contracts are a financial derivative representing the right but not obligation to purchase (**call** option) or sell (**put** option) a particular security or collection of securities (**basket**). 

## Data Sourcing  

We obtained the European options data from 2002 to 2021 using a WRDS subscription. Cleaning and pre-processing were performed to remove outliers and degenerated values.  
The data were not standardized or mapped into the unit interval, as we believe that normal standardization procedures would obstruct our model from learning fluctuations and trends in the data.  
However, relevant functions are implemented in `preprocess.py` and we can experiment with it easily.

WRDS does not provide data of basket options, therefore, we synthesize basket options from vanilla options. Further details are in `heston_basket_helper.py`.

## Methodology   

We have implemented and experimented with various machine learning algorithms, including the standard multilayer perceptron, Bayesian Neural Network [[1]](#1), Mixture Density Network [[2]](#2), and Support Vector Machine for regression [[3]](#3).  

Each of those methods has its strengths as well as weakness.  

For support vector machine, its trainning time varies significantly with the parameters set but it can provide relatively low errors compared to other benchmarks like Black Scholes and Heston model.

For probabilistic machine learning methods, while their training takes longer to converge, they can provide a confidence level for estimation.

For standard multilayer perceptron, its training takes only a relatively short amount of time thanks to modern computational advancements, but it can only provide a point estimate.  

Benchmarks for the Black Scholes and Heston model are run on Apple M1 with 8.00 GB of RAM.

Support vector machine is tuned using the `GridSearchCV` tuner in `scikit-learn` using 3-Folds cross-validator and is run on Apple M2 with 8.00 GB of RAM.  

All neural network models are tuned using the `RandomSearch` tuner in `Tensorflow` and trained for 100 epochs using data obtained above on Google Colab using an Nvidia Tesla T4 GPU.  

## Benchmarks    

#### Vanilla options
We provide some benchmarks for our methods. MSE stands for mean squared error and MAPE stands for mean absolute percentage error. The time for neural networks includes the training and testing time.  

<center>
| Methods       | MSE           | MAPE   | Time        |
| ------------- | ------------- |--------| ------------|
| Black-Scholes | 0.2216        | 101.78%| 0.021s      |
| MLP           | 0.0035        | 24.30% | 337.99s     |
| sinMLP        | 0.0034        | 24.18% | 517.98s     |
| BNN           | 0.0148        | 44.12% | 140.74s     |
| MDN           | 0.0048        | 32.21% | 165.01s     |
| SVM           | 0.0053        | 40.05% | 1463.64s    |
</center>


- MLP: Multilayer Perceptron, 5 layers with `[50,50,50,50,50]` neurons, using exponential linear unit activation  

- sinMLP: the same MLP as above but using $\sin$ activation function in an attempt to better model the fluctuations

- BNN: Bayesian Neural Network, 3 layers with `[300,100,100]` neurons with fixed normal priors and trainable normal posterior, using sigmoid activation

- MDN: Mixture Density Network, 5 layers with `[280,50,50,210]` neurons, using sigmoid activation with 70 components

- SVM: Support Vector Machine for regression using radial basis function as kernel  

The neural network models are tuned with a built-in `BayesianOptimization` tuner in `Tensorflow`.  
Both MLP models are trained for 100 epochs with a batch size of 32, while the BNN and MDN are trained for 500 epochs with a batch size of 128 to alleviate the effect of noisy gradients.   
The comparison with the Heston model using Monte Carlo simulation is excluded, as applying it to our data is infeasible due to the extremely long runtime.   

#### Basket options

We used Monte Carlo Heston Benchmark for basket options. As the data for basket options are synthetic, we only compared the time efficiency.

Monte Carlo Heston Benchmark:

|Basket Size|Feature Vector Length|Generating Inputs Time|Heston Pricing Time|Saving Time|Total Time Taken|
| ------------- | ------------- |--------| ------------|------------- |--------| 
|1|9|0.047|5.395|0.010|5.454|
|4	|15	|0.036|21.748|	0.014|	21.804|
|7	|21	|0.036|	37.925|	0.019|	37.990|
|10|	27	|0.036|54.141|0.024	|54.215|
|13|	33|	0.036|70.365	|0.027	|70.447|
|16|	39|	0.037|88.827|0.040	|88.945|

Multilayer Perceptron:
<p align="center">
<img src="./assets/NN training and testing time.png">
</p>
<p align="center">

Comparison:
<p align="center">
<img src="./assets/Comparison between Heston and NN.png">
</p>
<p align="center">

MLP structures used for different basket sizes:

|Basket Size|Number of Layers|Units| Activation Function|Batch normalization | 
| ------------- | ------------- | ------------- | ------------- | ------------- | 
|1|5|[6,6,7,7,7]	|tanh|True | 
|4|5|[7,5,5,5,5]	|sigmoid|  False | 
|7|2|[7,5]	|elu| False|
|10|5|[5,5,5,5,5]	|sigmoid|  False|
|13|3|[6,5,6]	|elu|  False|
|16|1|[6]	|relu|  False|

We used the built-in `RandomSearch`, `Hyperband` and `BayesianOptimization` tuners in `Tensorflow`. MLP models for basket sizes 1, 7 and 13 are tuned with `RandomSearch` tuner, basket sizes of 4 and 10 are tuned with `BayesianOptimization` and basket size of 16 is tuned with `Hyperband`. All MLP models are trained for 10 epochs with a batch size of 32. Further details are in `experimenting_basket.ipynb`. 

## Discussions and Outlooks   

It appears that machine learning methods are indeed suitable for pricing options. Once a neural network has been trained, it can be deployed and inference can be done in a relatively short time even on a CPU. Moreover, neural network models can leverage the computing power of modern GPUs, which speed up their training significantly compared to SVMs due to parallelization. 



## References  

<!-- <Using APA style for citation from semantic scholar> -->
<a id = "1">[1]</a>
Neal, R.M. (1995). Bayesian Learning for Neural Networks.

<a id = "2">[2]</a>
Bishop, C.M. (1994). Mixture density networks.  

<a id = "3">[3]</a>
Drucker, H., Burges, C.J., Kaufman, L., Smola, A., & Vapnik, V.N. (1996). Support Vector Regression Machines. NIPS.
